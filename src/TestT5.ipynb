{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: Required library version not found: libsbitsandbytes_cpu.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/hongy/miniconda3/envs/zljin_LLMs/lib/python3.9/site-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628d5977c52d4243aad63c31feacc440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType, PeftConfig, PeftModel\n",
    "from peft.peft_model import PeftModelForSeq2SeqLM\n",
    "\n",
    "# peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/public/home/hongy/zljin/FT_flanT5xl/Pretrained/flan_t5_xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/public/home/hongy/zljin/FT_flanT5xl/Pretrained/flan_t5_xl\")\n",
    "# model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 3\n",
      "tensor([[   3,    7,   17,    7,  115, 7142,  536,   10,   27,   31,   51,  352,\n",
      "           12,   43, 3832, 8988,    5, 7142,  357,   10,   27,   56, 3989, 2495,\n",
      "           48, 2272,    5,    1],\n",
      "        [   3,    7,   17,    7,  115, 7142,  536,   10,   27,   56, 3989, 3832,\n",
      "           48, 2272,    5, 7142,  357,   10,   27,   31,   51,  352,   12,   43,\n",
      "         3832, 8988,    5,    1],\n",
      "        [   3,    7,   17,    7,  115, 7142,  536,   10,   27,   31,   51,  352,\n",
      "           12,   43, 3832, 8988,    5, 7142,  357,   10,   27,   56, 3989, 2495,\n",
      "           48, 2272,    5,    1]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_text = [\"stsb sentence1: I'm going to have chicken tonight. sentence2: I will cook fish this evening.\",\n",
    "              \"stsb sentence1: I will cook chicken this evening. sentence2: I'm going to have chicken tonight.\",\n",
    "              \"stsb sentence1: I'm going to have chicken tonight. sentence2: I will cook fish this evening.\"]\n",
    "\n",
    "tokenized_dict = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = tokenized_dict['input_ids']\n",
    "attention_mask = tokenized_dict['attention_mask']\n",
    "\n",
    "bs_size = input_ids.shape[0]\n",
    "print(\"batch_size:\", bs_size)\n",
    "decoder_start_token = 0\n",
    "decoder_input_ids = torch.tensor([[decoder_start_token] for _ in range(bs_size)])\n",
    "print(input_ids)\n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, use_cache=False, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
      "torch.Size([3, 32128])\n",
      "tensor([[-11.3935,  -5.9171],\n",
      "        [ -0.4979,  -6.3665],\n",
      "        [-11.3935,  -5.9171]], grad_fn=<IndexBackward0>)\n",
      "tensor([1, 0, 1])\n",
      "torch.Size([3, 1])\n",
      "tensor([[1],\n",
      "        [0],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "logits = outputs[0].squeeze(1)\n",
    "print(logits.shape)\n",
    "res1 = logits[:, [4273, 150]]\n",
    "print(res1)\n",
    "res = torch.argmax(res1, dim=-1)\n",
    "print(res)\n",
    "res = res.unsqueeze(-1)\n",
    "print(res.shape)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/hongy/miniconda3/envs/zljin_LLMs/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, use_cache=True, num_beams=5, \n",
    "# \t\t\t\tlength_penalty=0.6, max_new_tokens=1, repetition_penalty=2.0, decoder_start_token_id=decoder_start_token)\n",
    "\n",
    "generated_ids = model.generate(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[150,   1]])\n"
     ]
    }
   ],
   "source": [
    "no_ids = tokenizer(\"no\", return_tensors='pt').input_ids\n",
    "print(no_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4273,    1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generated_ids)\n",
    "\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_and_cache_examples(args, tokenizer, evaluate=False, max_q = None, max_a = None):\n",
    "    if not evaluate:\n",
    "        type_path = f'train{args.train_name}'\n",
    "        logger.info(\"Loading training data from %s\", type_path)\n",
    "    else:\n",
    "        type_path = 'dev'   # DIFF HERE\n",
    "    dataset = TrainDataset(tokenizer, data_dir=args.data_dir, type_path=type_path, max_source_length=args.max_src_len, max_target_length=args.max_tgt_len)\n",
    "    return dataset\n",
    "eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True, max_q=max_q, max_a=max_a)\n",
    "if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "    os.makedirs(eval_output_dir)\n",
    "\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "# Note that DistributedSampler samples randomly\n",
    "eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=eval_dataset.collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zljin_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
